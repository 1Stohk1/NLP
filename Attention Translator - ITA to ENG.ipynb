{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1Stohk1/NLP/blob/main/Attention%20Translator%20-%20ITA%20to%20ENG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hlvTwt6wW0QZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a924c7c-62dd-4821-97ba-9bb02a15680e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Neural machine translation with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwtNgENbx2g"
      },
      "source": [
        "This notebook trains a sequence to sequence (seq2seq) model for Italian to English translation based on [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025v5). This is an advanced example that assumes some knowledge of:\n",
        "\n",
        "* Sequence to sequence models\n",
        "* How the GRUs network works\n",
        "* What is the Loung's attention\n",
        "\n",
        "While this methodology is outdated this is still a good starting point to acknowledge on the effectiveness of the Enc-Dec with Attention, despite the actual SoA with the Transformers.\n",
        "\n",
        "After training the model in this notebook, you will be able to input a Spanish sentence, such as \"Sei ancora a casa?\", and return the English translation: \"*are you still at home?*\"\n",
        "\n",
        "The resulting model is exportable as a `tf.saved_model`, so it can be used in other TensorFlow environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAmSR1FaqKrl"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGFTkuRvzWqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7018df0-1e38-4dc5-f61c-dc4aa82af67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text==2.8.*\n",
            "  Downloading tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.21.6)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.5.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (14.0.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.26.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.47.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.1.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.2.0)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.8.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting telepot\n",
            "  Downloading telepot-12.7.tar.gz (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from telepot) (1.24.3)\n",
            "Requirement already satisfied: aiohttp>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from telepot) (3.8.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (4.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (1.7.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (2.1.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp>=3.0.0->telepot) (2.10)\n",
            "Building wheels for collected packages: telepot\n",
            "  Building wheel for telepot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for telepot: filename=telepot-12.7-py3-none-any.whl size=57961 sha256=a818cf0fafdccd1dd8041843c473e50fb45161949bbc4b958c50d6a74fba73a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/4d/ea/8781198a69408b6c37d47d902d7bbe5541969eddf5290e6b2e\n",
            "Successfully built telepot\n",
            "Installing collected packages: telepot\n",
            "Successfully installed telepot-12.7\n"
          ]
        }
      ],
      "source": [
        "!pip install \"tensorflow-text==2.8.*\"\n",
        "!pip install telepot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import telepot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPJ9J7iPUchc"
      },
      "outputs": [],
      "source": [
        "use_builtins = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjUROhJfH3ML"
      },
      "source": [
        "## The data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puE_K74DIE9W"
      },
      "source": [
        "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
        "\n",
        "```\n",
        "May I borrow this book?\tPosso prendere in prestito questo libro?\n",
        "```\n",
        "\n",
        "They have a variety of languages available, but we'll use the English-Italian dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfodePkj3jEa"
      },
      "source": [
        "### Download and prepare the dataset\n",
        "\n",
        "For convenience, we've hosted a copy of this dataset on Google Drive. After downloading the dataset, here are the steps we'll take to prepare the data:\n",
        "\n",
        "1. Add a *start* and *end* token to each sentence.\n",
        "2. Clean the sentences by removing special characters.\n",
        "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
        "4. Pad each sentence to a maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRVATYOgJs1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a79a9dd5-1005-4879-8d24-8b43bf363c92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://drive.google.com/u/0/uc?id=1KoF4vyAHw6hjXdkJ9j7ybAWeKYS1osW7&export=download\n",
            "4481024/4479576 [==============================] - 0s 0us/step\n",
            "4489216/4479576 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Download the file\n",
        "import pathlib\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'ita.zip', origin='https://drive.google.com/u/0/uc?id=1KoF4vyAHw6hjXdkJ9j7ybAWeKYS1osW7&export=download',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/'ita.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHn4Dct23jEm"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "\n",
        "  inp = [inp for targ, inp in pairs]\n",
        "  targ = [targ for targ, inp in pairs]\n",
        "\n",
        "  return targ, inp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTbSbBz55QtF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "targ, inp = load_data(path_to_file)\n",
        "\n",
        "targ_train, targ_test, inp_train, inp_test = train_test_split(targ, inp, test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH_dPY8TRp3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2db7d3c-4052-42a8-bdc3-3a7497315c65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doubtless there exists in this world precisely the right woman for any given man to marry and vice versa; but when you consider that a human being has the opportunity of being acquainted with only a few hundred people, and out of the few hundred that there are but a dozen or less whom he knows intimately, and out of the dozen, one or two friends at most, it will easily be seen, when we remember the number of millions who inhabit this world, that probably, since the earth was created, the right man has never yet met the right woman.\n"
          ]
        }
      ],
      "source": [
        "print(targ[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfVWx3WaI5Df"
      },
      "source": [
        "From these arrays of strings you can create a `tf.data.Dataset` of strings that shuffles and batches them efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqHsArVZ3jFS"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(inp)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp_train, targ_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc6-NK1GtWQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "967bf9e7-1ef8-4bc0-91ba-feca22a2723a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"L'allenatore \\xc3\\xa8 frustrato?\" b'Siete pronti a volare?'\n",
            " b'Grazie per non avermelo detto.' b'Non pensi che ce ne dovremmo andare?'\n",
            " b'Tom Jackson \\xc3\\xa8 uno dei miei cabarettisti preferiti.'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b'Is the coach frustrated?' b'Are you ready to fly?'\n",
            " b'Thanks for not telling me.' b\"Don't you think we should leave?\"\n",
            " b'Tom Jackson is one of my favorite comedians.'], shape=(5,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for example_input_batch, example_target_batch in dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCoxLcuN3bwv"
      },
      "source": [
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kwdPcHvzz_a"
      },
      "source": [
        "One of the goals of this tutorial is to build a model that can be exported as a `tf.saved_model`. To make that exported model useful it should take `tf.string` inputs, and return `tf.string` outputs: All the text processing happens inside the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOQ5n55X4uDB"
      },
      "source": [
        "#### Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upKhKAMK4zzI"
      },
      "source": [
        "The model is dealing with multilingual text with a limited vocabulary. So it will be important to standardize the input text.\n",
        "\n",
        "The first step is Unicode normalization to split accented characters and replace compatibility characters with their ASCII equivalents.\n",
        "\n",
        "The `tensorflow_text` package contains a unicode normalize operation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD0e-DWGQ2Vo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ac7c58b-a14f-4aab-bfa5-369e33a573e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"Com'\\xc3\\xa8 il tempo oggi?\"\n",
            "b\"Com'e\\xcc\\x80 il tempo oggi?\"\n"
          ]
        }
      ],
      "source": [
        "example_text = tf.constant(\"Com'è il tempo oggi?\")\n",
        "\n",
        "print(example_text.numpy())\n",
        "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hTllEjK6RSo"
      },
      "source": [
        "Unicode normalization will be the first step in the text standardization function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chTF5N885F0P"
      },
      "outputs": [],
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accecented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UREvDg3sEKYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a31a7b78-aee6-4655-e4e8-aef352cdf282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Com'è il tempo oggi?\n",
            "[START] come il tempo oggi ? [END]\n"
          ]
        }
      ],
      "source": [
        "print(example_text.numpy().decode())\n",
        "print(tf_lower_and_split_punct(example_text).numpy().decode())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q-sKsSI7xRZ"
      },
      "source": [
        "#### Text Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aKn8qd37abi"
      },
      "source": [
        "This standardization function will be wrapped up in a `tf.keras.layers.TextVectorization` layer which will handle the vocabulary extraction and conversion of input text to sequences of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "outputs": [],
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kbC6ODP8IK_"
      },
      "source": [
        "The `TextVectorization` layer and many other preprocessing layers have an `adapt` method. This method reads one epoch of the training data, and works a lot like `Model.fix`. This `adapt` method initializes the layer based on the data. Here it determines the vocabulary:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmsI1Yql8FYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6838ebcc-f655-46ce-d8fb-a1a727d7ec33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'tom', '?', 'non', 'e', 'di']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "input_text_processor.adapt(inp)\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "input_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kGjIFjX8_Wp"
      },
      "source": [
        "That's the Italian `TextVectorization` layer, now build and `.adapt()` the English one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlC4xuZnKLBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6117e9b-f44c-4037-85fd-a313db566967"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'tom', 'you', 'i', 'to', '?']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "output_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor.adapt(targ)\n",
        "output_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQqlP_s9eIv"
      },
      "source": [
        "Now these layers can convert a batch of strings into a batch of token IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KZxj8IrNZ9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48dee2e2-cfbe-4781-d16c-6d173d9e3e4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
              "array([[   2,    1,    8, 3047,    6,    3,    0,    0,    0,    0],\n",
              "       [   2,   63, 1034,   10, 1546,    6,    3,    0,    0,    0],\n",
              "       [   2,  422,   18,    7,    1,   62,    4,    3,    0,    0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "example_tokens = input_text_processor(example_input_batch)\n",
        "example_tokens[:3, :10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA9rUn9G9n78"
      },
      "source": [
        "The `get_vocabulary` method can be used to convert token IDs back to text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98g9rcxGQY0I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c310f80b-9da7-492c-b705-d6f01e750d9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[START] [UNK] e frustrato ? [END]       '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
        "tokens = input_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "## The encoder/decoder model\n",
        "\n",
        "The following diagram shows an overview of the model. At each time-step the decoder's output is combined with a weighted sum over the encoded input, to predict the next word. The diagram and formulas are from [Luong's paper](https://arxiv.org/abs/1508.04025v5).\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzQWx2saImMV"
      },
      "source": [
        "Before getting into it define a few constants for the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a9uNz3-IrF-"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 256\n",
        "units = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blNgVbLSzpsr"
      },
      "source": [
        "### The encoder\n",
        "\n",
        "Start by building the encoder, the blue part of the diagram above.\n",
        "\n",
        "The encoder:\n",
        "\n",
        "1. Takes a list of token IDs (from `input_text_processor`).\n",
        "3. Looks up an embedding vector for each token (Using a `layers.Embedding`).\n",
        "4. Processes the embeddings into a new sequence (Using a `layers.GRU`).\n",
        "5. Returns:\n",
        "  * The processed sequence. This will be passed to the attention head.\n",
        "  * The internal state. This will be used to initialize the decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding for each token.\n",
        "    vectors = self.embedding(tokens)\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence.\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "    # 4. Returns the new sequence and its state.\n",
        "    return output, state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RIPHh4O9ixB"
      },
      "source": [
        "The encoder returns its internal state so that its state can be used to initialize the decoder.\n",
        "\n",
        "It's also common for an RNN to return its state so that it can process a sequence over multiple calls. You'll see more of that building the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45xM_Gl1MgXY"
      },
      "source": [
        "### The attention head\n",
        "\n",
        "The decoder uses attention to selectively focus on parts of the input sequence.\n",
        "The attention takes a sequence of vectors as input for each example and returns an \"attention\" vector for each example. This attention layer is similar to a `layers.GlobalAveragePoling1D` but the attention layer performs a _weighted_ average.\n",
        "\n",
        "Let's look at how this works:\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/images/attention_equation_1.jpg?raw=1\" alt=\"attention equation 1\" width=\"800\">\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/images/attention_equation_2.jpg?raw=1\" alt=\"attention equation 2\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX2JsKzzzgZ5"
      },
      "source": [
        "Where:\n",
        "\n",
        "* $s$ is the encoder index.\n",
        "* $t$ is the decoder index.\n",
        "* $\\alpha_{ts}$ is the attention weights.\n",
        "* $h_s$ is the sequence of encoder outputs being attended to (the attention \"key\" and \"value\" in transformer terminology).\n",
        "* $h_t$ is the the decoder state attending to the sequence (the attention \"query\" in transformer terminology).\n",
        "* $c_t$ is the resulting context vector.\n",
        "* $a_t$ is the final output combining the \"context\" and \"query\".\n",
        "\n",
        "The equations:\n",
        "\n",
        "1. Calculates the attention weights, $\\alpha_{ts}$, as a softmax across the encoder's output sequence.\n",
        "2. Calculates the context vector as the weighted sum of the encoder outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNA5GeHHPsGL"
      },
      "source": [
        "Last is the $score$ function. Its job is to calculate a scalar logit-score for each key-query pair. There are two common approaches:\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/images/attention_equation_4.jpg?raw=1\" alt=\"attention equation 4\" width=\"800\">\n",
        "\n",
        "This tutorial uses [Bahdanau's additive attention](https://arxiv.org/pdf/1409.0473.pdf). TensorFlow includes implementations of both as `layers.Attention` and\n",
        "`layers.AdditiveAttention`. The class below handles the weight matrices in a pair of `layers.Dense` layers, and calls the builtin implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "momiE59lXo6U"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "\n",
        "    # From Eqn. (4), `W1@ht`.\n",
        "    w1_query = self.W1(query)\n",
        "\n",
        "    # From Eqn. (4), `W2@hs`.\n",
        "    w2_key = self.W2(value)\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ638eHN4iCK"
      },
      "source": [
        "### The decoder\n",
        "\n",
        "The decoder's job is to generate predictions for the next output token.\n",
        "\n",
        "1. The decoder receives the complete encoder output.\n",
        "2. It uses an RNN to keep track of what it has generated so far.\n",
        "3. It uses its RNN output as the query to the attention over the encoder's output, producing the context vector.\n",
        "4. It combines the RNN output and the context vector using Equation 3 (below) to generate the \"attention vector\".\n",
        "5. It generates logit predictions for the next token based on the \"attention vector\".\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/images/attention_equation_3.jpg?raw=1\" alt=\"attention equation 3\" width=\"800\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZsQJMqNmg_L"
      },
      "source": [
        "Here is the `Decoder` class and its initializer. The initializer creates all the necessary layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erYvHIgAl8kh"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # For Step 1. The embedding layer convets token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # For step 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "\n",
        "    # For step 5. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUTfYHmfmwKH"
      },
      "source": [
        "The `call` method for this layer takes and returns multiple tensors. Organize those into simple container classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WfSIb2sArRT"
      },
      "outputs": [],
      "source": [
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NChkl2KrnV2y"
      },
      "source": [
        "Here is the implementation of the `call` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJOi5btHAPNK"
      },
      "outputs": [],
      "source": [
        "def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "\n",
        "  # Step 1. Lookup the embeddings\n",
        "  vectors = self.embedding(inputs.new_tokens)\n",
        "\n",
        "  # Step 2. Process one step with the RNN\n",
        "  rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "  # Step 3. Use the RNN output as the query for the attention over the\n",
        "  # encoder output.\n",
        "  context_vector, attention_weights = self.attention(\n",
        "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
        "  context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
        "  attention_vector = self.Wc(context_and_rnn_output)\n",
        "\n",
        "  # Step 5. Generate logit predictions:\n",
        "  logits = self.fc(attention_vector)\n",
        "\n",
        "  return DecoderOutput(logits, attention_weights), state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay_mTMPfnb2a"
      },
      "outputs": [],
      "source": [
        "Decoder.call = call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arTOBklcFTiC"
      },
      "source": [
        "The **encoder** processes its full input sequence with a single call to its RNN. This implementation of the **decoder** _can_ do that as well for efficient training. But this tutorial will run the decoder in a loop for a few reasons:\n",
        "\n",
        "* Flexibility: Writing the loop gives you direct control over the training procedure.\n",
        "* Clarity: It's possible to do masking tricks and use `layers.RNN`, or `tfa.seq2seq` APIs to pack this all into a single call. But writing it out as a loop may be clearer. \n",
        "  * Loop free training is demonstrated in the [Text generation](text_generation.ipynb) tutiorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6xyru86m914"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now that you have all the model components, it's time to start training the model. You'll need:\n",
        "\n",
        "- A loss function and optimizer to perform the optimization.\n",
        "- A training step function defining how to update the model for each input/target batch.\n",
        "- A training loop to drive the training and save checkpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "### Define the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "outputs": [],
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5AgEBh2S404"
      },
      "source": [
        "### Implement the training step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_G20Te1XSmJ"
      },
      "source": [
        "Start with a model class, the training process will be implemented as the `train_step` method on this model. See [Customizing fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit) for details.\n",
        "\n",
        "Here the `train_step` method is a wrapper around the `_train_step` implementation which will come later. This wrapper includes a switch to turn on and off `tf.function` compilation, to make debugging easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWIyuy71TkJT"
      },
      "outputs": [],
      "source": [
        "class TrainTranslator(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    return self._tf_train_step(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i0i1x6jwsLm"
      },
      "source": [
        "Overall the implementation for the `Model.train_step` method is as follows:\n",
        "\n",
        "1. Receive a batch of `input_text, target_text` from the `tf.data.Dataset`.\n",
        "2. Convert those raw text inputs to token-embeddings and masks. \n",
        "3. Run the encoder on the `input_tokens` to get the `encoder_output` and `encoder_state`.\n",
        "4. Initialize the decoder state and loss. \n",
        "5. Loop over the `target_tokens`:\n",
        "   1. Run the decoder one step at a time.\n",
        "   2. Calculate the loss for each step.\n",
        "   3. Accumulate the average loss.\n",
        "6. Calculate the gradient of the loss and use the optimizer to apply updates to the model's `trainable_variables`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngBjFw4BU5G7"
      },
      "source": [
        "The `_preprocess` method, added below, implements steps #1 and #2: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlYE68wzXoA8"
      },
      "outputs": [],
      "source": [
        "def _preprocess(self, input_text, target_text):\n",
        "\n",
        "  # Convert the text to token IDs\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  target_tokens = self.output_text_processor(target_text)\n",
        "\n",
        "  # Convert IDs to masks.\n",
        "  input_mask = input_tokens != 0\n",
        "\n",
        "  target_mask = target_tokens != 0\n",
        "\n",
        "  return input_tokens, input_mask, target_tokens, target_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHy6hzStrgjQ"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._preprocess = _preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3kvbcArc2y-"
      },
      "source": [
        "The `_train_step` method, added below, handles the remaining steps except for actually running the decoder: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs_gsISsYPpY"
      },
      "outputs": [],
      "source": [
        "def _train_step(self, inputs):\n",
        "  input_text, target_text = inputs  \n",
        "\n",
        "  (input_tokens, input_mask,\n",
        "   target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "  max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Encode the input\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "    # Initialize the decoder's state to the encoder's final state.\n",
        "    # This only works if the encoder and decoder have the same number of\n",
        "    # units.\n",
        "    dec_state = enc_state\n",
        "    loss = tf.constant(0.0)\n",
        "\n",
        "    for t in tf.range(max_target_length-1):\n",
        "      # Pass in two tokens from the target sequence:\n",
        "      # 1. The current input to the decoder.\n",
        "      # 2. The target for the decoder's next prediction.\n",
        "      new_tokens = target_tokens[:, t:t+2]\n",
        "      step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                             enc_output, dec_state)\n",
        "      loss = loss + step_loss\n",
        "\n",
        "    # Average the loss over all non padding tokens.\n",
        "    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "  # Apply an optimization step\n",
        "  variables = self.trainable_variables \n",
        "  gradients = tape.gradient(average_loss, variables)\n",
        "  self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  # Return a dict mapping metric names to current value\n",
        "  return {'batch_loss': average_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGwWHIxLrjGR"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._train_step = _train_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7g40o-mXyt5"
      },
      "source": [
        "The `_loop_step` method, added below, executes the decoder and calculates the incremental loss and new decoder state (`dec_state`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VrzgwztXzYJ"
      },
      "outputs": [],
      "source": [
        "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "  input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "  # Run the decoder one step.\n",
        "  decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                               enc_output=enc_output,\n",
        "                               mask=input_mask)\n",
        "\n",
        "  dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "\n",
        "  # `self.loss` returns the total for non-padded tokens\n",
        "  y = target_token\n",
        "  y_pred = dec_result.logits\n",
        "  step_loss = self.loss(y, y_pred)\n",
        "\n",
        "  return step_loss, dec_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj3I7VULrk1R"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._loop_step = _loop_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFUsTKQx0jaH"
      },
      "outputs": [],
      "source": [
        "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "def _tf_train_step(self, inputs):\n",
        "  return self._train_step(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-bgU59jrztQ"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._tf_train_step = _tf_train_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Emgfgh4tAmJt"
      },
      "outputs": [],
      "source": [
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpObfY22IddU"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "While there's nothing wrong with writing your own custom training loop, implementing the `Model.train_step` method, as in the previous section, allows you to run `Model.fit` and avoid rewriting all that boiler-plate code. \n",
        "\n",
        "This tutorial only trains for a couple of epochs, so use a `callbacks.Callback` to collect the history of batch losses, for plotting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7m4mtnj80sq"
      },
      "outputs": [],
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):  \n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQd_esVVoSf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5e94f8-0135-4dfd-d499-30ee372f0120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "4428/4428 [==============================] - 1037s 232ms/step - batch_loss: 1.1292\n",
            "Epoch 2/5\n",
            "4428/4428 [==============================] - 1003s 226ms/step - batch_loss: 0.5070\n",
            "Epoch 3/5\n",
            "4428/4428 [==============================] - 994s 224ms/step - batch_loss: 0.3747\n",
            "Epoch 4/5\n",
            "4428/4428 [==============================] - 993s 224ms/step - batch_loss: 0.2961\n",
            "Epoch 5/5\n",
            "4428/4428 [==============================] - 985s 222ms/step - batch_loss: 0.2504\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f032a6680d0>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "train_translator.fit(dataset, epochs=5,\n",
        "                     callbacks=[batch_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38rLdlmtQHCm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "aaaf6a4f-85e9-41b7-ca2d-fcf7d35eeaf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'CE/token')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(batch_loss.logs)\n",
        "plt.ylim([0, 3])\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('CE/token')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0S_O_RzHmfe"
      },
      "source": [
        "The visible jumps in the plot are at the epoch boundaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n",
        "\n",
        "Now that the model is trained, implement a function to execute the full `text => text` translation.\n",
        "\n",
        "For this the model needs to invert the `text => token IDs` mapping provided by the `output_text_processor`. It also needs to know the IDs for special tokens. This is all implemented in the constructor for the new class. The implementation of the actual translate method will follow.\n",
        "\n",
        "Overall this is similar to the training loop, except that the input to the decoder at each time step is a sample from the decoder's last prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO-CLL1LVBbM"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string(tf.constant('[START]'))\n",
        "    self.end_token = index_from_string(tf.constant('[END]'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBQzFZ9uWU79"
      },
      "outputs": [],
      "source": [
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b59PN-UxqYrU"
      },
      "source": [
        "### Convert token IDs to text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-razg3Aso737"
      },
      "source": [
        "The first method to implement is `tokens_to_text` which converts from token IDs to human readable text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IjwKTwtmdFf"
      },
      "outputs": [],
      "source": [
        "def tokens_to_text(self, result_tokens):\n",
        "  result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "\n",
        "  result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                       axis=1, separator=' ')\n",
        "\n",
        "  result_text = tf.strings.strip(result_text)\n",
        "  return result_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "912aV0K7r90w"
      },
      "outputs": [],
      "source": [
        "Translator.tokens_to_text = tokens_to_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krBuAapkqNs9"
      },
      "source": [
        "Input some random token IDs and see what it generates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWCMHdoS32QN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcfdf3c6-b519-4fee-87f2-d26c6404eda0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'seventeen brainwashed', b'his objection', b'material bananas',\n",
              "       b'lack passport', b'last closed'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "example_output_tokens = tf.random.uniform(\n",
        "    shape=[5, 2], minval=0, dtype=tf.int64,\n",
        "    maxval=output_text_processor.vocabulary_size())\n",
        "translator.tokens_to_text(example_output_tokens).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample from the decoder's predictions"
      ],
      "metadata": {
        "id": "AVHN1rqwhOQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes the decoder's logit outputs and samples token IDs from that distribution:"
      ],
      "metadata": {
        "id": "Ycp4H45fhUg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(self, logits, temperature):\n",
        "\n",
        "  token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "  logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "  if temperature == 0.0:\n",
        "    new_tokens = tf.argmax(logits, axis=-1)\n",
        "  else: \n",
        "    logits = tf.squeeze(logits, axis=1)\n",
        "    new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                        num_samples=1)\n",
        "  \n",
        "\n",
        "  return new_tokens"
      ],
      "metadata": {
        "id": "D3jQhCbohWKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Translator.sample = sample"
      ],
      "metadata": {
        "id": "qQHAwtLqhX_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEWIKFIJ2HWM"
      },
      "source": [
        "### Implement the translation loop\n",
        "\n",
        "Here is a complete implementation of the text to text translation loop.\n",
        "\n",
        "This implementation collects the results into python lists, before using `tf.concat` to join them into tensors.\n",
        "\n",
        "This implementation statically unrolls the graph out to `max_length` iterations.\n",
        "This is okay with eager execution in python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxYXf3GNKKLS"
      },
      "source": [
        "Run it on a simple input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd2rgyHwVVrv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a4cf999-6427-4403-e47f-2959679944a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 89 µs, sys: 10 µs, total: 99 µs\n",
            "Wall time: 105 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "input_text = tf.constant([\n",
        "    'Fa molto freddo qui.', # \"It's really cold here.\"\n",
        "    'Questa è la mia vita.', # \"This is my life.\"\"\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-6cFyqeUPQm"
      },
      "source": [
        "If you want to export this model you'll need to wrap this method in a `tf.function`. This basic implementation has a few issues if you try to do that:\n",
        "\n",
        "1. The resulting graphs are very large and take a few seconds to build, save or load.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  { display-mode: \"form\" }\n",
        "def translate_symbolic(self,\n",
        "                       input_text,\n",
        "                       *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "\n",
        "  batch_size = tf.shape(input_text)[0]\n",
        "\n",
        "  # Encode the input\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  enc_output, enc_state = self.encoder(input_tokens)\n",
        "  # Initialize the decoder\n",
        "  dec_state = enc_state\n",
        "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "  \n",
        "  # Initialize the accumulators\n",
        "  result_tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n",
        "  attention = tf.TensorArray(tf.float32, size=1, dynamic_size=True)\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "  for t in tf.range(max_length):\n",
        "    dec_input = DecoderInput(\n",
        "        new_tokens=new_tokens, enc_output=enc_output, mask=(input_tokens != 0))\n",
        "\n",
        "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "    attention = attention.write(t, dec_result.attention_weights)\n",
        "\n",
        "    new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "    # If a sequence produces an `end_token`, set it `done`\n",
        "    done = done | (new_tokens == self.end_token)\n",
        "    # Once a sequence is done it only produces 0-padding.\n",
        "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "    # Collect the generated tokens\n",
        "    result_tokens = result_tokens.write(t, new_tokens)\n",
        "\n",
        "    if tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Convert the list of generated token ids to a list of strings.\n",
        "  result_tokens = result_tokens.stack()\n",
        "  result_tokens = tf.squeeze(result_tokens, -1)\n",
        "  result_tokens = tf.transpose(result_tokens, [1, 0])\n",
        "\n",
        "  result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "  if return_attention:\n",
        "    attention_stack = attention.stack()\n",
        "\n",
        "    attention_stack = tf.squeeze(attention_stack, 2)\n",
        "\n",
        "    attention_stack = tf.transpose(attention_stack, [1, 0, 2])\n",
        "\n",
        "    return {'text': result_text, 'attention': attention_stack}\n",
        "  else:\n",
        "    return {'text': result_text}"
      ],
      "metadata": {
        "id": "MzdwaEMgAKi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngywxv1WYO_O"
      },
      "outputs": [],
      "source": [
        "Translator.translate = translate_symbolic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX6EF8KtYh20"
      },
      "outputs": [],
      "source": [
        "@tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "def tf_translate(self, input_text):\n",
        "  return self.translate(input_text)\n",
        "\n",
        "Translator.tf_translate = tf_translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq8d40RKYoJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3957166-e2af-4c45-aa7e-1d7fb959e82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.8 s, sys: 280 ms, total: 2.08 s\n",
            "Wall time: 3.47 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "result = translator.tf_translate(\n",
        "    input_text = input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bleu Score\n",
        "\n",
        "Now we want to find out how our model is performing using the Bleu score, with progressive N-Grams from 1 till 4. The score will be compared with other kind of model to find out which is the best one in the translation of sentences."
      ],
      "metadata": {
        "id": "CwJ2h8NHzHya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-gram cumulative BLEU\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "score = 0\n",
        "for target, input in zip(targ_test, inp_test):\n",
        "  input_text = tf.constant([\n",
        "                            input\n",
        "  ])\n",
        "  result = translator.tf_translate(input_text)\n",
        "  candidate = result['text'][0].numpy().decode()\n",
        "\n",
        "  candidate = candidate.replace(',', ' ,').replace('.', ' .').split(\" \")\n",
        "  reference = target.lower().replace(',', ' ,').replace('.', ' .').replace(\"'\", \"\").split(' ')\n",
        "\n",
        "  score += sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "score = score/len(targ_test)\n",
        "print(score)"
      ],
      "metadata": {
        "id": "g8i3DYDMzgCG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f76110-b2b5-4a5e-df37-dd40e97bf500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.854948959770019e-159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notifying the bot on Telegram\n",
        "token = '5453584270:AAHIZnD7IoHRWzSmzQR_yGU2T-676IVmSoo'\n",
        "receiver = 806724607\n",
        "\n",
        "bot = telepot.Bot(token)\n",
        "\n",
        "bot.sendMessage(receiver, \"The training of the Translator has been completed and it reached the score:\" + str(score))"
      ],
      "metadata": {
        "id": "3_o7XA---BSl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "377e1197-564e-4c3e-ca55-d08279f8dcae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ProtocolError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    383\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 104] Connection reset by peer",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-3bdb7190f641>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtelepot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreceiver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The training of the Translator has been completed the reached score is:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/telepot/__init__.py\u001b[0m in \u001b[0;36msendMessage\u001b[0;34m(self, chat_id, text, parse_mode, disable_web_page_preview, disable_notification, reply_to_message_id, reply_markup)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;34m\"\"\" See: https://core.telegram.org/bots/api#sendmessage \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sendMessage'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rectify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     def forwardMessage(self, chat_id, from_chat_id, message_id,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/telepot/__init__.py\u001b[0m in \u001b[0;36m_api_request\u001b[0;34m(self, method, params, files, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api_request_with_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/telepot/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(req, **user_kw)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0muser_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0muser_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# `fn` must be thread-safe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/request.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;31m# Read retry?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mread\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMA9Pp71nzH9"
      },
      "source": [
        "## Export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rLMNOmsKoXe"
      },
      "source": [
        "Once you have a model you're satisfied with you might want to export it as a `tf.saved_model` for use outside of this python program that created it.\n",
        "\n",
        "Since the model is a subclass of `tf.Module` (through `keras.Model`), and all the functionality for export is compiled in a `tf.function` the model should export cleanly with `tf.saved_model.save`:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP2dNtEXJPEL"
      },
      "source": [
        "Now that the function has been traced it can be exported using `saved_model.save`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyvxT5V0_X5B"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(translator, '/content/drive/MyDrive/Colab/HLT/TraduttoreITA',\n",
        "                    signatures={'serving_default': translator.tf_translate})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}